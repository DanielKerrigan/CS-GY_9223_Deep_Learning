{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapted from `rlcard/examples/leduc_holdem_dqn_pytorch.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.insert(0, os.path.abspath('./rlcard'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import rlcard\n",
    "import json\n",
    "import numpy as np\n",
    "from DQNAgent import DQNAgent\n",
    "from rlcard.agents import RandomAgent, CFRAgent\n",
    "from rlcard.utils import set_global_seed, tournament\n",
    "from rlcard.utils import Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_global_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_env = rlcard.make('limit-holdem', config={'seed': 0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Settings A - 1m hands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after 1 million hands\n",
    "dqn_agent_3 = DQNAgent(\n",
    "    eval_env.action_num,\n",
    "    eval_env.state_shape[0],\n",
    "    hidden_neurons=[1024, 512, 1024, 512]\n",
    ")\n",
    "\n",
    "dqn_agent_3.load('./models/limit_holdem_dqn/03/step-5474450.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Settings A - 0.5m hands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after 500,000 hands\n",
    "dqn_agent_3_half = DQNAgent(\n",
    "    eval_env.action_num,\n",
    "    eval_env.state_shape[0],\n",
    "    hidden_neurons=[1024, 512, 1024, 512]\n",
    ")\n",
    "\n",
    "dqn_agent_3_half.load('./models/limit_holdem_dqn/03/step-2500210.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Settings B - 1m hands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after ~1 million hands\n",
    "dqn_agent_5_end = DQNAgent(\n",
    "    eval_env.action_num,\n",
    "    eval_env.state_shape[0],\n",
    "    hidden_neurons=[1024, 512, 1024, 512]\n",
    ")\n",
    "\n",
    "dqn_agent_5_end.load('./models/limit_holdem_dqn/05/step-5179089.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Settings B - 0.5m hands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after ~half million hands\n",
    "dqn_agent_5 = DQNAgent(\n",
    "    eval_env.action_num,\n",
    "    eval_env.state_shape[0],\n",
    "    hidden_neurons=[1024, 512, 1024, 512]\n",
    ")\n",
    "\n",
    "dqn_agent_5.load('./models/limit_holdem_dqn/05/step-2533731.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Settings C - 2m hands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after ~2 million hands\n",
    "dqn_agent_6_end = DQNAgent(\n",
    "    eval_env.action_num,\n",
    "    eval_env.state_shape[0],\n",
    "    hidden_neurons=[1024, 512, 1024, 512]\n",
    ")\n",
    "\n",
    "dqn_agent_6_end.load('./models/limit_holdem_dqn/06/step-10014402.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Settings C - 1m hands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after ~1 million hands\n",
    "dqn_agent_6_middle = DQNAgent(\n",
    "    eval_env.action_num,\n",
    "    eval_env.state_shape[0],\n",
    "    hidden_neurons=[1024, 512, 1024, 512]\n",
    ")\n",
    "\n",
    "dqn_agent_6_middle.load('./models/limit_holdem_dqn/06/step-5186110.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('rlcard/rlcard/games/limitholdem/card2index.json', 'r') as file:\n",
    "    card2index = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['call', 'raise', 'fold', 'check']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_env.actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = np.zeros(228)\n",
    "\n",
    "obs[card2index['S2']] = 1\n",
    "obs[card2index['C7']] = 1\n",
    "obs[208] = 1\n",
    "obs[213] = 1\n",
    "obs[218] = 1\n",
    "obs[223] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([nan, nan, nan, nan], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_input = torch.tensor(obs,\n",
    "                           dtype=torch.float,\n",
    "                           device=dqn_agent_3.device)\n",
    "\n",
    "dqn_agent_3.policy_net.eval()\n",
    "dqn_agent_3.policy_net(model_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0539,  0.4436, -5.2679,  0.1856], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_input = torch.tensor(obs,\n",
    "                           dtype=torch.float,\n",
    "                           device=dqn_agent_3_half.device)\n",
    "\n",
    "dqn_agent_3_half.policy_net.eval()\n",
    "dqn_agent_3_half.policy_net(model_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.4005,  0.4427, -0.8425,  0.3784], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_input = torch.tensor(obs,\n",
    "                           dtype=torch.float,\n",
    "                           device=dqn_agent_5.device)\n",
    "\n",
    "dqn_agent_5.policy_net.eval()\n",
    "dqn_agent_5.policy_net(model_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.8466, -1.4755, -0.9655, -0.6745], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_input = torch.tensor(obs,\n",
    "                           dtype=torch.float,\n",
    "                           device=dqn_agent_5_end.device)\n",
    "\n",
    "dqn_agent_5_end.policy_net.eval()\n",
    "dqn_agent_5_end.policy_net(model_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.7728,  1.7508, -0.8454,  1.5301], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_input = torch.tensor(obs,\n",
    "                           dtype=torch.float,\n",
    "                           device=dqn_agent_6_middle.device)\n",
    "\n",
    "dqn_agent_6_middle.policy_net.eval()\n",
    "dqn_agent_6_middle.policy_net(model_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.1130,  0.6298, -0.8841,  0.5140], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_input = torch.tensor(obs,\n",
    "                           dtype=torch.float,\n",
    "                           device=dqn_agent_6_end.device)\n",
    "\n",
    "dqn_agent_6_end.policy_net.eval()\n",
    "dqn_agent_6_end.policy_net(model_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate models against each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_agent = RandomAgent(action_num=eval_env.action_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(agent1, agent2, n=100000):\n",
    "    eval_env.set_agents([agent1, agent2])\n",
    "    return tournament(eval_env, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A (0.5m) vs B (0.5m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.74562, 0.74562]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval(dqn_agent_3_half, dqn_agent_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A (0.5m) vs random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.19958, -2.19958]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval(dqn_agent_3_half, random_agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B (0.5m) vs random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.45081, -2.45081]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval(dqn_agent_5, random_agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B (0.5m) vs B (1m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.766985, -1.766985]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval(dqn_agent_5, dqn_agent_5_end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C (1m) vs C (2m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.71339, -0.71339]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval(dqn_agent_6_middle, dqn_agent_6_end)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C (1m) vs random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.00511, -2.00511]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval(dqn_agent_6_middle, random_agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B (0.5m) vs. C (1m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.2025, 0.2025]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval(dqn_agent_5, dqn_agent_6_middle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C (1m) vs B (0.5m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.21637, -0.21637]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval(dqn_agent_6_middle, dqn_agent_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C (1m) vs C (1m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.04212, -0.04212]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval(dqn_agent_6_middle, dqn_agent_6_middle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B (0.5m) vs B (0.5m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.00265, 0.00265]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval(dqn_agent_5, dqn_agent_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "random vs random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.00573, -0.00573]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval(random_agent, random_agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Action Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "env = rlcard.make('limit-holdem', config={'seed': 0, 'record_action': True})\n",
    "env.set_agents([dqn_agent_6_middle, dqn_agent_6_middle])\n",
    "\n",
    "counts = Counter()\n",
    "total_reward = 0\n",
    "\n",
    "for i in range(100000):\n",
    "    trajectories, payoffs = env.run(is_training=False)\n",
    "    if trajectories[-1]:\n",
    "        final_transition = trajectories[-1][-1]\n",
    "        final_state = final_transition[0]\n",
    "        total_reward += final_transition[2]\n",
    "        actions = final_state['action_record']\n",
    "        counts.update(x[1] for x in actions)\n",
    "    else:\n",
    "        print(trajectories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'call': 390944, 'raise': 606571, 'check': 31384})"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2489.0"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
